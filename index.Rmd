---
title: "A Data Exploration of My Girlfriend's and My Spotify Top 50 Songs"
author: "Ding Si Han | Spring 2022"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: journal
    self_contained: false
---


```{r setup, include=FALSE}
library(flexdashboard)
library(ggrepel)
library(ggthemes)
library(tidyverse)
library(compmus)
library(gridExtra)
library(spotifyr)
library(plotly)

sihan_df <- read.csv(file="./data/sihan.csv")
megan_df <- read.csv(file="./data/megan.csv")

combined_df <- rbind(sihan_df, megan_df)
combined_df$mode <- as.factor(combined_df$mode)

summary(combined_df)


```




### Reflective music: A deep chordal dive into **Cornerstone**

```{r}
# Megan's Songs
song_black_bear <- "2BuMmK2Yguu6vQf1PJ2xQu"
song_maybe <- "5tOUTNlLFd5u5xPt3unOVp"
song_carry_you <- "0u4rkpmNtgcFxYHepnVF4v"

# Si Han's Songs
song_yet_not_i <- "439tGS9rVbyTjj5SmneD56"
song_ta_shuo <- "632VyMrvhsHIsO4zq9khts"
song_cornerstone <- "23STcG7v9HNwdqnROFSS8n"



circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis(song_cornerstone) %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


***

Cornerstone is a classic Christian Worship song that reflects a very typical chord progression based around 4 chords. C major, A minor, F major and G major. It is very interesting to observe the chordogram as it can be observed that the song has 5 main sections. The first sections is up til 115 seconds and is marked by a dominant region in the middle of the y-axis signifying the main chord progression that runs through the song. 

However, what is very interesting is the varying degrees of yellow in the rest of the chords for the duration of the song. The first section is marked by a quite a significant shade of yellow. This then fades from 115 to 230 seconds. This is because whilst both the first and second section includes the verse and chorus, the second region builds and incoporates more instruments, especially the electric guitar filler lines which are not in the chord progression, thereby diluting the intensity of the chords.

The song then falls to a solemn reflective mood, culminating at 270 to 280 seconds, where we hear most of the instrument fades out except the C major chord holding. This is reflected in the strong blue shade in this region, where the chord progression comes out stronger due to the other instrument fading. As the song builds again from 330 to 390 seconds, the yellowness fades again, similar to what we observe in the second region. The song then finally ends with the C major chord being held again, explaining the strong yellow regions for the other chords. 

This song typifies a song that I personally like that is calming and reflective, and the variations of the intensity of the music is brought out in this chordogram by the intensity of yellow around the major chord progression. It typifies this style of emotional worship music that brings peace and that I enjoy and my reflective personality, something that my girlfriend tells me I embody.



### Is there such a thing as work-from-home music: A dive into **Carry You**
```{r}



bzt <-
  get_tidy_audio_analysis(song_carry_you) %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

bzt %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***

Carry You is by one of Megan's favourite artists Novo Amor and for good reason. This song Carry You displays dreamy vibes that builds into a hopeful crescendo towards the end of the song. The start is marked by a metallic ring which dies down, and may be reflected in the yellow fade from the first 20 seconds in the seventh dimension. Of note, this is followed with a flatness represented by the third dimension between 30s to 45s. 

The song then builds up before fading, which is depicted in the 1st dimension which represents loudness. This wave like fluctuation between 50s to 140s creates an air of floatiness and creates the ideas of possibilities. This then builds to a sudden high pulsating volumes at 150s to 160s which uplifts the listeners and sends hopes the our hearts. This high volumes then sustains to build the positivity within the listen before a gentle slow and fade away. 

This light yet uplifting song is perfect for a study or work session, typifying a song that would pair well with Megan's typical routines at work. For a listener that is at the stage of working life, and especially work from home, Novo Amor's Carry You brilliantly sings a encouraging tone to forge ahead with a day's worth of work


### A tale of **two individuals**: Can music define an individual?

**The Story**

Someone once said music is the way to speak to the soul. Yet, what makes music so intriguing is how personal each individual's choices are. Some may like rock Music while others may prefer classical music. Is this just a personal preference, are does that indicate more than meets the eye about an individual's personality. To find out, I will be comparing my girlfriend, Megan, and my top songs to examine what are some of the similarities and differences. As people with similar personalities yet different hobbies and interests, it will be intriguing to examine where our music choices converge and elements where they differ. Hence, the key research question that I will seek to address is to what extent are our musical preferences different and whether it encapsulate the differences in our personalities and lifestyle.

Megan prefers more of lo-fi, indie music, especially those with chill vibes. I often listen to emotional and worshipful music. Hence, it will be intersting to especially examine these features and explore how this reveals more about each of us individuals.

1. Danceability
2. Valence
3. Speechiness
4. Instrumentalness 

Personally, both of us listen to our music primarily on Spotify. In fact, more than 90% of the music that we listen to is on Spotify itself. As a result, this would make the Spotify API and these playlists pretty accurate and representative of our musical tastes. Interestingly, Megan listens to music while doing work typically, while I listen to most of my music during leisure. Hence, this will also open the door to examining if the features of our music can reveal the different motivations for listening to music.

**Megan's Songs**

*Typical Songs:*

1. [Black bear](https://open.spotify.com/track/2BuMmK2Yguu6vQf1PJ2xQu?si=b0a95a9035b64ca4)
2. [Maybe](https://open.spotify.com/track/5tOUTNlLFd5u5xPt3unOVp?si=449cf9c5ff244d50)
3. [Carry you](https://open.spotify.com/track/0u4rkpmNtgcFxYHepnVF4v?si=08bfe49d1774442e)

The typical songs found in Megan's playlist are indie folk with slow vibes to the songs.

*Atypical Songs*

1. [We’re good](https://open.spotify.com/track/1diS6nkxMQc3wwC4G1j0bh?si=f426a1a347e5402a)

We're good is a song that is unlike the rest of the songs, with louder, faster and more beats to it.


**Si Han's Songs**

*Typical Songs:*

1. [Yet Not I but Through Christ in Me](https://open.spotify.com/track/439tGS9rVbyTjj5SmneD56?si=ee649e2657664910)
2. [Ta Shuo](https://open.spotify.com/track/632VyMrvhsHIsO4zq9khts?si=bb87253cfc654d1b)

Typical songs in the playlist would include Christian Worship songs and Mandopop songs, which comprise approximately 50% of the playlist each.

*Atypical Songs*

1. [Light Rain](https://open.spotify.com/track/3BA4y1ENVDOeT7XP82sk09?si=bfae484a270d4d00)

Light rain is an ambient sound track of rain drops that are useful for falling asleep. This will be an interesting track to analyse due to the nature of the sounds and there being no distinct pitch of the sounds.


***

Insert Playlist Embedding here (currently still not working), new

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DXaQm3ZVg9Z2X?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


```






### Why a boring song exists: analysis of a **strange outlier**

```{r}

light_rain = "439tGS9rVbyTjj5SmneD56"
yet_not_i = "439tGS9rVbyTjj5SmneD56"
shuo_hao_bu_ku = "56wVfJKtnwlSZtC4NVgIrf"

wood <-
  get_tidy_audio_analysis(light_rain) %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

chromagram <- wood %>%
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

```{r}
## The Tallis Scholars
tallis <-
  get_tidy_audio_analysis("3BA4y1ENVDOeT7XP82sk09") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
## La Chapelle Royale
chapelle <-
  get_tidy_audio_analysis("3BA4y1ENVDOeT7XP82sk09") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

dtw <- compmus_long_distance(
  tallis %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
  chapelle %>% mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Light Rain", y = "Light Rain") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)

grid.arrange(chromagram, dtw, ncol = 2) 
```

***

The Rain Song is one of the interesting audio tracks in the list of songs for the reason that it is the only one without any melody or pitch. That being said, it is quite interesting to observe that the chromagram displays the sounds predominantly at the C pitch. This is likely to be less of an intentional feature, and more of a feature of the frequency of the rain drops corresponding to this pitch.

The Dynamic Time Wrapping also sheds insight into the fact that there are no significant dynamics in the audio track. Instead, it is fairly constant and monotonous, with the exception of the bright line at 140 seconds, which corresponds to a period with a bit more fluctuations in the intensity of rain.

This outlier song is a cool find within the haystack of music and one that is unusual not only to my playlist, yet most spotify songs. However, it brings out the fact that Spotify can be a space not only for listening to music, but even routines such as sounds to fall alseep to, as used in my case.





### Positivity and soulfulness, can these be **musically quantified**?

```{r}
ggplot(combined_df, aes(valence, instrumentalness, color=mode)) +
  geom_point() +
  facet_wrap(~listener) +
  scale_color_manual(labels = c("Minor", "Major"), values = c("red", "blue")) +
# geom_text(aes(x=0.14, label=ifelse((instrumentalness>0.5),as.character(name),'')))
  geom_text_repel(aes(label=ifelse((instrumentalness>0.85),as.character(name),''))) 
# theme_economist()
```

*** 

Here, in the area of valence, we observe that the range of songs span across the whole spectrum to up to 0.8, while mine ranges up to 0.5. This is actually quite an accrurate reflection of our personalities, Megan being an upbeat and cheerful individual, while I am quite a reflective especially with regard to music, which helps to unwind my thoughts and feelings.

What is extremely insightful is also the range of the instrumentalness of the songs. Megan's songs again span the whole spectrum, with one significant song being an outlier from the rest: Hollywood. The wide range of instrumentalness reflects that the types of songs she listens to contains both those with voices and those without. This diversity is something that can be contrasted against my songs, which are very low on instrumentalness. This is in line with the genres that I listen to: worship and mandopop songs, which both comes with vocals inside typically.

One exception to this is the song Light Rain, which is a audio track of rain songs I listen to to fall asleep. Here, we can clearly tell the Spotify API was spot on to indicate a score of near 1.0 on instrumentalness, given that this track has absolutely no vocals inside.


